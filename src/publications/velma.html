{% extends 'publication-layout.html' %} {% set pageTitle = 'Modulate Platform
Demo' %} {% block content %}

<div class="ids__wrapper">
  <div class="ids__space XL"></div>

  <h1 class="M">#1 AI Model for Understanding Voice Conversations</h1>
  <div class="ids__space L"></div>

  <p class="loud intro">
    <a href="#">Conversation Understanding Benchmark.</a>
  </p>
  <div class="ids__space L"></div>
  <div class="ids__sequence XL gap-L">
    <div class="ids__sequence-item">
      <div class="tmp-hero-scatterplot">hero scatterplot</div>
    </div>
    <div class="ids__sequence-item">
      <p>Dataset</p>
      <p>Methodology</p>
      <p>Score factor</p>
    </div>
  </div>
  <div class="ids__space M"></div>
  <h2>Velma 2.0 vs. Leading AI research companies</h2>
  <div class="ids__space S"></div>
  <table class="velma-tmp-table">
    <thead>
      <tr>
        <th>Model Name</th>
        <th>Accuracy</th>
        <th>Cost</th>
        <th>Speed</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>OpenAI GPT5 family</td>
        <td>Velma 30% more accurate</td>
        <td>Velma 10x less costly</td>
        <td>5x faster</td>
      </tr>
      <tr>
        <td>Google - Gemini3 family</td>
        <td>Velma 27% more accurate</td>
        <td></td>
        <td></td>
      </tr>
      <tr>
        <td>xAi Grok 4 family</td>
        <td></td>
        <td></td>
        <td>2x faster</td>
      </tr>
    </tbody>
  </table>
  <div class="ids__space M"></div>
  <h2>Velma 2.0 component models are also industry-leading.</h2>
  <p>
    As part of the ELM (link) architecture, Velma uses dozens of proprietary
  </p>

  <div class="ids__space M"></div>
  <div class="ids__sequence L gap-L">
    <div class="ids__sequence-item">
      <p>Transcription</p>

      <div class="tmp-hero-scatterplot">scatterplot</div>
    </div>
    <div class="ids__sequence-item">
      <p>Synthetic Voice / Deepfake</p>

      <div class="tmp-hero-scatterplot">scatterplot</div>
    </div>
    <div class="ids__sequence-item">
      <p>Emotion</p>
      <div class="tmp-hero-scatterplot">scatterplot</div>
    </div>
  </div>

  <p>Add other information</p>

  <p>ADD links - Velma, Mini-site, white paper</p>

  <div class="ids__space M"></div>
  <h3>Whitepaper</h3>
  <p>
    Comparisons against [Claude Sonnet 4, DeepSeek-Prover-V2-671B, and
    Kimina-Prover-72B.]
  </p>

  <div class="ids__space M"></div>
  <h3>About the data</h3>

  <p>
    Conversational Voice Understanding is an open benchmark designed to evaluate
    the entire content of an audio conversation. Audio conversations contain
    significantly more contexutal nuances than text, including, emotion,
    sentiment, tone, volume, rhapsody, timbre, multiple meanings and more.
  </p>

  <div class="ids__space M"></div>
  <h3>Methodology</h3>
  <p>
    The conversational understanding benchmark evaluates the ability of models
    to answer audio-reasoning based questions. All models are provided with an
    audio dataset of 30 to 60 minute conversations. The models are asked to
    answer hundreds of questions about the conversations. No additional
    information is provided to the model. The output file contains answers that
    are evaluated by an AI model judge. The judge labels the answer as correct
    or incorrect. Explain Accuracy UNITS and what are the cost units? $ per ____
  </p>
</div>

{% endblock %}
